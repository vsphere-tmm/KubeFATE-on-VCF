{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component.dataio import DataIO\n",
    "from pipeline.component.homo_lr import HomoLR\n",
    "from pipeline.component.reader import Reader\n",
    "from pipeline.interface.data import Data\n",
    "from pipeline.interface.model import Model\n",
    "from pipeline.component.evaluation import Evaluation\n",
    "from pipeline.component.scale import FeatureScale\n",
    "from pipeline.utils.tools import load_job_config\n",
    "from pipeline.runtime.entity import JobParameters\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-monday",
   "metadata": {},
   "source": [
    "# Simple Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Examples/Pipeline/notebooks/creditcard_guest.csv')\n",
    "\n",
    "robust_scale = RobustScaler()\n",
    "df['Scaled_Amount'] = robust_scale.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['Scaled_Time'] = robust_scale.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "IDs = [i + 1 for i in range(len(df))]\n",
    "\n",
    "Scaled_Amount = df['Scaled_Amount']\n",
    "Scaled_Time = df['Scaled_Time']\n",
    "df.drop(['Scaled_Amount','Scaled_Time'], axis=1, inplace=True)\n",
    "df.insert(0, 'id', IDs)\n",
    "df.insert(1, 'Scaled_Amount', Scaled_Amount)\n",
    "df.insert(2, 'Scaled_Time', Scaled_Time)\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('/Examples/Pipeline/notebooks/creditcard_guest_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-interim",
   "metadata": {},
   "source": [
    "# Upload Data To Local FATE Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "guest = 9999\n",
    "\n",
    "# spark + pulsar\n",
    "backend = 2\n",
    "work_mode = 1\n",
    "\n",
    "partition = 8\n",
    "\n",
    "guest_train_data = {\"name\": \"dataset_guest\", \"namespace\": f\"experiment\"}\n",
    "\n",
    "guest_eval_data = {\"name\": \"dataset_guest\", \"namespace\": f\"experiment\"}\n",
    "\n",
    "pipeline_upload = PipeLine().set_initiator(role=\"guest\", party_id=guest).set_roles(guest=guest)\n",
    "# add upload data info\n",
    "# original csv file path\n",
    "pipeline_upload.add_upload_data(file=os.path.join('/Examples/Pipeline/notebooks/creditcard_guest_scaled.csv'),\n",
    "                                table_name=guest_train_data[\"name\"],\n",
    "                                namespace=guest_train_data[\"namespace\"],\n",
    "                                head=1, partition=partition)\n",
    "# upload all data\n",
    "pipeline_upload.upload(work_mode=work_mode, backend=backend, drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-vault",
   "metadata": {},
   "source": [
    "# Define the Components of Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parties config\n",
    "guest = 9999 \n",
    "host = 10000\n",
    "arbiter = 10000\n",
    "\n",
    "host_train_data = {\"name\": \"dataset_host\", \"namespace\": f\"experiment\"}\n",
    "\n",
    "host_eval_data = {\"name\": \"dataset_host\", \"namespace\": f\"experiment\"}\n",
    "\n",
    "# define Reader components to read in data\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "# configure Reader for guest\n",
    "reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=guest_train_data)\n",
    "# configure Reader for host\n",
    "reader_0.get_party_instance(role='host', party_id=host).component_param(table=host_train_data)\n",
    "\n",
    "reader_1 = Reader(name=\"reader_1\")\n",
    "reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=guest_eval_data)\n",
    "reader_1.get_party_instance(role='host', party_id=host).component_param(table=host_eval_data)\n",
    "# define DataIO components\n",
    "dataio_0 = DataIO(name=\"dataio_0\", with_label=True, output_format=\"dense\", label_name='Class')  # start component numbering at 0\n",
    "dataio_1 = DataIO(name=\"dataio_1\", with_label=True, output_format=\"dense\", label_name='Class')  # start component numbering at 0\n",
    "\n",
    "param = {\n",
    "    \"penalty\": \"L2\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"tol\": 1e-05,\n",
    "    \"alpha\": 0.01,\n",
    "    \"max_iter\": 5,\n",
    "    \"early_stop\": \"diff\",\n",
    "    \"batch_size\": 50000,\n",
    "    \"learning_rate\": 0.15,\n",
    "    \"validation_freqs\": 1,\n",
    "    \"init_param\": {\n",
    "        \"init_method\": \"zeros\"\n",
    "    },\n",
    "    \"encrypt_param\": {\n",
    "        \"method\": None\n",
    "    },\n",
    "    \"cv_param\": {\n",
    "        \"n_splits\": 4,\n",
    "        \"shuffle\": True,\n",
    "        \"random_seed\": 33,\n",
    "        \"need_cv\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "homo_lr_0 = HomoLR(name='homo_lr_0', **param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-malta",
   "metadata": {},
   "source": [
    "# Compose and Submit the Pipeline of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pipeline\n",
    "pipeline = PipeLine()\n",
    "\n",
    "# set job initiator\n",
    "pipeline.set_initiator(role=\"guest\", party_id=guest)\n",
    "# set participants information\n",
    "pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n",
    "\n",
    "# add components to pipeline, in order of task execution\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(reader_1)\n",
    "\n",
    "pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n",
    "pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data),\n",
    "                       model=Model(dataio_0.output.model))\n",
    "\n",
    "homo_lr_0 = HomoLR(name='homo_lr_0', **param)\n",
    "pipeline.add_component(homo_lr_0, data=Data(train_data=dataio_0.output.data,\n",
    "                                                validate_data=dataio_1.output.data))\n",
    "\n",
    "evaluation_0 = Evaluation(name=\"evaluation_0\", eval_type=\"binary\")\n",
    "evaluation_0.get_party_instance(role='host', party_id=host).component_param(need_run=False)\n",
    "pipeline.add_component(evaluation_0, data=Data(data=homo_lr_0.output.data))\n",
    "\n",
    "# compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n",
    "pipeline.compile()\n",
    "\n",
    "# fit model\n",
    "job_parameters = JobParameters(backend=backend, work_mode=work_mode)\n",
    "pipeline.fit(job_parameters)\n",
    "# query component summary\n",
    "import json\n",
    "print(json.dumps(pipeline.get_component(\"homo_lr_0\").get_summary(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-taiwan",
   "metadata": {},
   "source": [
    "# Compose and Submit the Pipeline of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "# deploy pipeline model\n",
    "pipeline.deploy_component([dataio_0, homo_lr_0])\n",
    "\n",
    "predict_pipeline = PipeLine()\n",
    "# add data reader onto predict pipeline\n",
    "predict_pipeline.add_component(reader_1)\n",
    "# add selected components from train pipeline onto predict pipeline\n",
    "# specify data source\n",
    "predict_pipeline.add_component(pipeline,\n",
    "                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_1.output.data}))\n",
    "# run predict model\n",
    "predict_pipeline.predict(job_parameters)\n",
    "\n",
    "print(json.dumps(predict_pipeline.get_component(\"homo_lr_0\").get_summary(), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
